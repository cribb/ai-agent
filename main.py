import os
import sys

from dotenv import load_dotenv
from google import genai
from google.genai import types

from functions.call_function import call_function, available_functions
from config import MAX_AI_ITERATIONS, SYSTEM_PROMPT, MODEL_NAME


def main():

    load_dotenv()
    api_key = os.environ.get("GEMINI_API_KEY")
    client = genai.Client(api_key=api_key)

    # count = 0
    # for arg in sys.argv:
    #     print(f"Argument {count}: {arg}")
    #     count += 1

    try:
        prompt = sys.argv[1]
    except Exception:
        print('Error: No prompt provided.')
        print('\nAI Assistant (using Gemini)')
        print('usage: python main.py "AI prompt here"')
        print('example: "WTF does \'skibidi\' mean?"\n') 
        return sys.exit(1)

    
    verbose = "--verbose" in sys.argv

    messages = [
        types.Content(role="user", parts=[types.Part(text=prompt)]),
    ]
    
    gemini_meat()

    # The meat of the interaction with Gemini:
    # First, configure it...
    ai_config = types.GenerateContentConfig(tools=[available_functions], 
                                            system_instruction=SYSTEM_PROMPT)

    # Then, submit the query to Gemini and [wait for|return] the content it generates
    response = client.models.generate_content(model=MODEL_NAME,
                                              contents=messages,
                                              config=ai_config)
    
    # Spam console with content if we're being chatty
    print("---- ai-agent ----")
    if verbose:
        print(f"User prompt: {prompt}")
        print(f"Prompt tokens: {response.usage_metadata.prompt_token_count}")
        print(F"Response tokens: {response.usage_metadata.candidates_token_count}")

    # Add the response to the "conversation" stored in messages (for feedback)
    # candidates = response.candidates
    if response.candidates:
        count = 1
        for candidate in response.candidates:
            if verbose:
                print(f"Candidate {count}:\n ")
                print(f"--> contents: {candidate.content},\n")
            messages.append(candidate.content)
            count += 1

    # Examine the response(s).
    func_calls = response.function_calls

    # If the agent has made no function calls, return its text response
    if not func_calls:
        return response.text

    # Process functions that were run...
    func_responses = []
    for part in func_calls:
        result = call_function(part, verbose)

        print(f"result.parts[0] --> {result.parts[0]}")
        print(f"result.parts[0].function_response.response---> {result.parts[0].function_response.response} ")
        
        func_response = result.parts[0].function_response.response

        if not result.parts or not func_response:
            raise Exception("Function call did not return expected response.")            
        # user_message = types.Content        
        if verbose:
            print(f"-> {func_response.get('result', func_response)}")
        func_responses.append(func_response)
        
    if not func_responses:
        raise Exception('No function responses generated by Gemini, exiting...')
    
    return func_responses

def gemini_meat():
    pass    

if __name__ == "__main__":
    main()
